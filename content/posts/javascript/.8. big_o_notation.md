---
id: 8
title: "Big O Notation"
description: "An universal way to measure performance."
category: "Javascript"
last_updated: "30 November 2023"
---

You're probably thinking, yet another article talking about Big O and you'll be right, but right now i'm just trying to solidify my knowledge like this, so just deal with it.

## Measuring performance

When we are facing a problem we generally have a few potential solutions at hand but how do we know which one is best? To define best we should first define what's our goal, is it performance ? Or maybe it is readability, because both generally come at the cost of the other. Or is our goal to be the fastest considering the data we're dealing with is not big enought to matter away.
Honeslty all goals are fair but at some point you'll have to find the most performant solution and when you do, how?

Well, we could just time every solutions using [performance.now](https://developer.mozilla.org/en-US/docs/Web/API/Performance/now) right?

```js
function addUpTo(n) {
  let total = 0;
  for (let i = 1; i <= n; i++) {
    total += i;
  }
  return total;
}

let time1 = performance.now();
addUpTo(1000000);
let time2 = performance.now();
console.log(`Time passed: ${(t2 - t1) / 1000} seconds`);
```

Well, yes we could but i you run this snippet multiple time you'll notice that the result is slightly different each time, and if you coworker tried aswell he'd get different results.
So although the trend and the conclusion is probably the same, we lack precision and a universal way to mesure.

## There comes Big O

Instead of counting seconds between functions, we'll count the number of operations the computer as to perform. Let's do that :

```js
function addUpTo(n) {
  let total = 0; // 1 assignment
  for (let i = 1; i <= n; i++) {
    // 1 assignment; n assigments; n additions; n comparisons
    total += i; // n assignment; n addition
  }
  return total;
}
```

So we could say that the number of operations is **5n + 2**. But should we really care about all this mathy operations? If you think so, just consider that _regardless of the exact number, the number of operations grows proportionally with n_.

## Constants don't matter

Sure the result would defer significanlty if we had 100 items, but would it we had 100 000 items or a 1 000 000? Surely not, plus as the function grows in complexity our time to calculate big O would grow aswell, and we don't want to spend time on mathy things we just WANT TO CODE.

So we just focus on loops :

```js
function addUpTo(n) {
  let total = 0;
  for (let i = 1; i <= n; i++) {
    // loop
    total += i;
  }
  return total;
}
```

Here addUpTo time spent scales based on the number of items (n) so here Big O is O(n);

How about here?

```js
function addUpTo(n) {
  return (n * (n + 1)) / 2;
}
```

Do we see any loop here? nope only mathy operations, so here big O is O(1), the time spent is the same regardless of the input size.

What if we have a loop inside a loop?

```js
function bubbleSort(arr) {
  const n = arr.length;

  for (let i = 0; i < n; i++) {
    // Last i elements are already in place, so no need to check them
    for (let j = 0; j < n - i - 1; j++) {
      // Swap if the element found is greater than the next element
      if (arr[j] > arr[j + 1]) {
        [arr[j], arr[j + 1]] = [arr[j + 1], arr[j]];
      }
    }
  }
}
```

In our bubbleSort function we loop over each element in the provided array before iterating again to compare current element with adjacent element and swap both if the next element is greater that the current one.

This example counts two loops, leading to a quadratic growth rate as the input size increases, so our Big O notation here is O(nÂ²).

```js
function binarySearch(sortedArray, target) {
  let low = 0;
  let high = sortedArray.length - 1;

  while (low <= high) {
    const mid = Math.floor((low + high) / 2);

    if (sortedArray[mid] === target) {
      return mid; // Target found, return its index
    } else if (sortedArray[mid] < target) {
      low = mid + 1; // Target is in the right half
    } else {
      high = mid - 1; // Target is in the left half
    }
  }

  return -1; // Target not found
}
```

In our binarySearch function, we could say at first glance that it has a time complexity of o(n) but if we look closer we can see that with each iteration we reduce the search space by half, making it more efficient and instead resulting in a Big O notation of O(log n).

To close the loop we have to talk about the last common time complexity, O(n log n) there you might think, again some mathy operations we though we were passed that! Well we are, O(n log n)
essentially mean that we divide the array in halves O(log n) before merging them O(n), let see an example:

```js
function mergeSort(arr) {
  if (arr.length <= 1) {
    return arr; // Base case: arrays with 0 or 1 element are already sorted
  }

  // Split the array into two halves
  const middle = Math.floor(arr.length / 2);
  const leftHalf = arr.slice(0, middle);
  const rightHalf = arr.slice(middle);

  // Recursively sort each half
  const sortedLeft = mergeSort(leftHalf);
  const sortedRight = mergeSort(rightHalf);

  // Merge the sorted halves
  return merge(sortedLeft, sortedRight);
}

function merge(left, right) {
  let result = [];
  let leftIndex = 0;
  let rightIndex = 0;

  // Compare elements from the left and right arrays and merge them in sorted order
  while (leftIndex < left.length && rightIndex < right.length) {
    if (left[leftIndex] < right[rightIndex]) {
      result.push(left[leftIndex]);
      leftIndex++;
    } else {
      result.push(right[rightIndex]);
      rightIndex++;
    }
  }

  // If there are remaining elements in either the left or right array, append them
  return result.concat(left.slice(leftIndex), right.slice(rightIndex));
}
```

Without getting into too much details about this code, we essentially divide the input into halves until each sub arrays has 0 or 1 elements at which point we're sure it is sorted so we can merge the sub-arrays together until we have our whole array sorted.

So there we have it all the possible Big O notations:

## Two sides of the same coin

Apart from calculating time complexity we can also take care of space complexity which represents the amount of memory space an algorithm requires relative to the input size.
<!-- TODO: explain how to calculate space complexity -->
Let find out how we can calculate space complexity:

1. Variables: Each variable or constant used in the algorithm consumes memory. Count the number of variables and constants, and determine whether their space requirements are constant or dependent on the input size. A general rule of thumb: 
- primitives (booleans, numbers, undefined, null) are constantspace
- String require O(n) space where n is the string length
- Reference types are generally O(n) where n is the length (for arrays) or the number of keys (for objects)

2. Recursive Calls: If the algorithm involves recursion, consider the space used by the call stack. Each recursive call adds a new frame to the stack, and the space complexity depends on the maximum depth of the recursion.

3. Auxiliary Space: Consider any additional space used by the algorithm, such as temporary variables or arrays created during execution.

4. Input Space: If the input is not modified in place, consider the space required to store a copy of the input.


Let's take an example:

```js
function sum(arr) {
  let total = 0; // one number
  for (let i = 0; i < arr.length; i++) { // one number
    total += arr[i]
  }
  return total;
}
```

So basically only primitives which means O(1) space!


Let's take another example:
```js
function double(arr) {
  let newArr = [];
  for (let i = 0; i < arr.length; i++) {
    newArr.push(2 * arr[i])
  }
  return newArr;
}
```

On every iteration, we are pushing a new item in the array, which means the space complexity is directly linked to the input, which means O(n)! 

Now i guess you get the general idea, but i'm still intrigued what's the time complexity of our every day operations?

## Objects!

Before diving right in, i think you should pause for one moment and think about when should we use objects?
- When you don't need order
- When you need fast access / insertion and removal

So now that we acknowledged it, what's the time complexity of every method we use daily on objects? Well there you have it: 

- Insert : O(1), Easy no order so just add your key - value pair anywhere
- Removal: O(1), Easy again no order, delete the one you want!
- Search O(N), well you we have to loop on every element
- Access: O(1), If we have the key, the value is paired with it easy

What about methods?
- Object.keys; O(N) well yes you loop over each of them
- Object.values: O(N) Well yes again we loop over
- Object.entries: O(N) Same logic here
- HasOwnProperty: O(1) Same as access we have a key so we can retrieve its pair 

<!-- TODO Performance of objects in JS -->
## When are arrays slow?

Again before thinking about performance, we should ask ourselves when should we use them ?
- When you need order
- When you need fast access / removal and insert (but we'll see later that it depends on how)

So what the time complexity then:
- Insert : It depends on where you insert it
- Removal: It depends on from where you remove it
- Search O(N), well you still have to loop on every element
- Access: O(1), If we have the index it's kind of the same as object

So why does it depend?
```js
let names = ["Michael", "Melissa", "Andrea"] // indexes : 0, 1, 2
// Insert at the beginning, we have to reindex each element!
names = ["newNames", "Michael", "Melissa", "Andrea"] // index : 0, 1, 2, 3
// Insert at the end, we dont have to reindex
names = ["newNames", "Michael", "Melissa", "Andrea", "Test"] // index : 0, 1, 2, 3, 4
```

Thus insertion and removal are best done at the end for performance reason, so still based on the methods we use daily which one should we use?

Methods: 
- push : O(1) yes we add it at the end no reindexing needed
- pop : O(1) same here, we remove from the end
- shift: O(N) we are adding at the beginning so every element is reindexed
- unshift: O(N) same here, we are removing from the beginning
- concat: O(N) the array grows proportionnally to the input
- slice: O(N) we're are copying objects so the time grows propertionnaly to the number of items
- splice: O(N) allows us to remove/add at a certain location (if not at the beginning) it means reindexing
- sort: O(N log N) basically larger than O(N) because rather than just looping we have to compare, move items around etc...
- forEach/map/filter/reduce... : O(N) we loop on every item!